---
title: vLLM Qwen2.5-7B-Instruct 在 RTX 3090 部署紀錄
date: 2026-01-30
category: tech
tags: [vLLM, Qwen, RTX3090, LLM, 部署]
---

# vLLM Qwen2.5-7B-Instruct 在 RTX 3090 部署紀錄

## 摘要

成功在 ac-3090 (RTX 3090 24GB) 上部署 vLLM v0.14.1 運行 Qwen2.5-7B-Instruct 模型。
過程中遇到多個注意力後端相容性問題，最終使用 TRITON_ATTN 後端成功啟動。

## 環境資訊

| 項目 | 版本/規格 |
|------|-----------|
| GPU | NVIDIA RTX 3090 24GB |
| Driver | 590.48.01 |
| CUDA (PyTorch) | 12.8 |
| PyTorch | 2.9.1+cu128 |
| vLLM | 0.14.1 (V1 engine) |
| Triton | 3.5.1 |
| 模型 | Qwen/Qwen2.5-7B-Instruct (~15GB) |

## 排查過程

### 問題：vLLM 啟動後掛起

初始啟動時，vLLM 自動選擇 FLASH_ATTN 後端，但實際上 flash-attn 套件未安裝，
導致模型載入階段無限掛起（GPU 記憶體已分配但進程 0% CPU）。

### 嘗試的後端

| 後端 | 結果 | 原因 |
|------|------|------|
| FLASH_ATTN (預設) | ❌ 掛起 | flash-attn 套件未安裝 |
| XFORMERS | ❌ 失敗 | vLLM v0.14.1 不支援此後端 |
| TORCH_SDPA | ❌ 失敗 | 需要預先註冊 |
| FLASHINFER | ❌ 失敗 | 需要 nvcc/CUDA toolkit 做 JIT 編譯，系統未安裝 |
| **TRITON_ATTN** | ✅ 成功 | Triton 3.5.1 已安裝，不需要 nvcc |

### 關鍵發現

1. **vLLM v0.14.1 (V1 engine)** 的有效注意力後端與舊版不同
2. ac-3090 **未安裝 CUDA toolkit (nvcc)**，只有 NVIDIA driver + PyTorch 自帶的 CUDA runtime
3. 需要用 `HF_HUB_DISABLE_XET=1` 下載模型（xet 協議有問題）

## 成功啟動指令

```bash
nohup env VLLM_ATTENTION_BACKEND=TRITON_ATTN \
  python3 -m vllm.entrypoints.openai.api_server \
  --model Qwen/Qwen2.5-7B-Instruct \
  --host 127.0.0.1 \
  --port 8000 \
  --max-model-len 4096 \
  --gpu-memory-utilization 0.85 \
  --enforce-eager \
  --dtype float16 \
  > /tmp/vllm_startup.log 2>&1 &
```

## 測試結果

所有 6 項測試通過：

| 測試 | 結果 | 說明 |
|------|------|------|
| 健康檢查 | ✅ | HTTP 200 |
| 模型列表 | ✅ | Qwen/Qwen2.5-7B-Instruct, max_model_len=4096 |
| 中文推理 | ✅ | 正確回答機器學習問題（繁體中文） |
| 程式碼生成 | ✅ | 正確生成 is_prime() 函式 |
| 多輪對話 | ✅ | 正確計算 345/5=69 |
| GPU 記憶體 | ✅ | 使用 21.3GB / 空閒 2.8GB |

## 資源使用

- **VRAM**: 約 21.3 GB（85% 使用率設定）
- **推理後 GPU 利用率**: 0%（閒置時不佔 GPU 計算資源）

## 注意事項

1. 服務目前以 nohup 方式運行，重開機後需手動啟動
2. 考慮建立 systemd service 做自動啟動
3. 剩餘 VRAM 約 2.8GB，無法同時運行其他大型 GPU 任務
4. API 端點: `http://127.0.0.1:8000`（僅本機存取）

## 後續工作

- [ ] 建立 systemd service 自動啟動
- [ ] 整合到 Compute Plane 服務
- [ ] 效能基準測試（throughput, latency）
