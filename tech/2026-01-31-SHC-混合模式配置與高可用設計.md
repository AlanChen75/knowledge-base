---
title: SHC 混合模式配置與高可用設計
date: 2026-01-31
category: tech/ai-ml
tags: [SHC, LLM, vLLM, OpenAI, High-Availability, Cost-Optimization]
source: /home/ac-mac/super-happy-tests/HYBRID_MODE_CONFIG.md
status: design
---

# SHC 混合模式配置與高可用設計

## 摘要

Super Happy Coder (SHC) v3.3.0 支援混合模式 LLM Router:
- **HIGH tier**: OpenAI gpt-4.1-nano (複雜任務)
- **LOW tier**: 本地 vLLM Qwen2.5-7B (簡單任務)
- **目標**: 節省 70% API 成本,同時確保高可用性

**目前狀態**: LOW tier 也使用 OpenAI,並非真正混合模式
**待實作**: 動態切換系統,根據 3090 運作狀況自動調整路由

---

## 一、目前配置狀態

### 1.1 環境變數配置 (acmacmini2)

**路徑**: `~/workshop/super-happy-coder/.env`

**目前設定**:
```bash
# SHC Proxy
PORT=8081

# LLM Router 配置
LLM_HIGH_PROVIDER=openai
LLM_HIGH_MODEL=gpt-4.1-nano

LLM_LOW_PROVIDER=openai        # ❌ 應改為 vllm
LLM_LOW_MODEL=gpt-4.1-nano      # ❌ 應改為 Qwen/Qwen2.5-7B-Instruct

# Fallback 鏈
LLM_FALLBACK_CHAIN=openai,vllm
```

**問題**:
- LOW tier 也使用 OpenAI = 100% 外部成本
- 無法發揮 3090 GPU 的價值
- 未實現混合模式成本節省

### 1.2 理想配置

```bash
LLM_LOW_PROVIDER=vllm
LLM_LOW_MODEL=Qwen/Qwen2.5-7B-Instruct
```

**效果**:
- 70% 請求使用免費 vLLM → 節省 70% 成本
- 30% 複雜請求使用 OpenAI → 保證品質
- **估算節省**: $63/月 (假設每日 100 請求)

---

## 二、高可用性需求分析

### 2.1 核心需求

根據用戶要求:
> "要設計成可用參數切換的模式，隨時根據需求與3090的運作狀況動態調整，避免系統因為3090服務失能而當機"

**關鍵需求**:
1. **參數化切換**: 可透過 API 動態調整 tier 配置
2. **健康監控**: 持續監測 3090 服務狀態
3. **自動降級**: 3090 失效時自動切換到 OpenAI
4. **零停機**: 配置變更不需重啟服務
5. **防止當機**: 即使 3090 完全失能,系統仍可運作

### 2.2 失效場景

| 場景 | 影響 | 目前行為 | 理想行為 |
|------|------|----------|----------|
| 3090 vLLM 服務停止 | LOW tier 無法使用 | ❌ 請求失敗 | ✅ 自動 fallback 到 OpenAI |
| ac-3090 主機關機 | 所有 Compute APIs 失效 | ❌ 連線逾時 | ✅ 降級到全外部模式 |
| SSH Tunnel 斷線 | 無法連接 3090 | ❌ 連線錯誤 | ✅ 檢測後切換 |
| 3090 GPU 過熱 | 效能下降 | ⚠️ 緩慢回應 | ✅ 主動切換到 OpenAI |
| vLLM 記憶體不足 | OOM 錯誤 | ❌ 服務崩潰 | ✅ Circuit breaker 觸發 |

---

## 三、動態切換系統設計

### 3.1 系統架構

```
┌─────────────────────────────────────────────────────────────┐
│                    SHC Proxy (acmacmini2)                   │
│                                                             │
│  ┌─────────────────────────────────────────────────────┐   │
│  │              LLM Router Manager                     │   │
│  │                                                     │   │
│  │  ┌──────────────┐  ┌────────────────────────────┐  │   │
│  │  │ Config Store │  │   Health Monitor           │  │   │
│  │  │              │  │                            │  │   │
│  │  │ - HIGH tier  │  │ - 3090 health check       │  │   │
│  │  │ - LOW tier   │  │ - OpenAI rate limit       │  │   │
│  │  │ - Fallback   │  │ - Circuit breaker state   │  │   │
│  │  └──────────────┘  └────────────────────────────┘  │   │
│  │                                                     │   │
│  │  ┌──────────────────────────────────────────────┐  │   │
│  │  │       Routing Decision Engine               │  │   │
│  │  │                                              │  │   │
│  │  │  if tier == HIGH:                            │  │   │
│  │  │      → OpenAI (always)                       │  │   │
│  │  │  elif tier == LOW:                           │  │   │
│  │  │      if vllm_healthy:                        │  │   │
│  │  │          → vLLM (3090)                       │  │   │
│  │  │      else:                                   │  │   │
│  │  │          → OpenAI (fallback)                 │  │   │
│  │  └──────────────────────────────────────────────┘  │   │
│  └─────────────────────────────────────────────────────┘   │
│                                                             │
│  ┌─────────────────────────────────────────────────────┐   │
│  │              Management API                         │   │
│  │                                                     │   │
│  │  POST /api/v1/admin/llm/config/update              │   │
│  │  GET  /api/v1/admin/llm/health                     │   │
│  │  POST /api/v1/admin/llm/force-mode                 │   │
│  │  POST /api/v1/admin/llm/reload                     │   │
│  └─────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────┘
           │                                      │
           │ HIGH tier                            │ LOW tier
           ▼                                      ▼
    ┌─────────────┐                     ┌──────────────────┐
    │   OpenAI    │                     │  vLLM (3090)     │
    │ gpt-4.1-nano│◄────────────────────│  健康檢查        │
    └─────────────┘    Fallback         │  Circuit Breaker │
                                        └──────────────────┘
```

### 3.2 健康監控機制

#### 3.2.1 健康檢查端點

**實作位置**: `proxy.py` 新增 `HealthMonitor` 類別

```python
class HealthMonitor:
    def __init__(self):
        self.vllm_status = "unknown"  # healthy | degraded | unhealthy
        self.last_check = None
        self.failure_count = 0
        self.circuit_breaker_open = False

    async def check_vllm_health(self):
        """檢查 vLLM 服務健康狀態"""
        try:
            response = await httpx.get(
                "http://localhost:9000/health",
                timeout=3.0
            )
            if response.status_code == 200:
                data = response.json()
                gpu_available = data.get("gpu", {}).get("available", False)

                if gpu_available:
                    self.vllm_status = "healthy"
                    self.failure_count = 0
                    self.circuit_breaker_open = False
                else:
                    self.vllm_status = "degraded"
            else:
                self._handle_failure()

        except Exception as e:
            self._handle_failure()
            logger.warning(f"vLLM health check failed: {e}")

        self.last_check = datetime.now()

    def _handle_failure(self):
        """處理健康檢查失敗"""
        self.failure_count += 1
        if self.failure_count >= 3:
            self.vllm_status = "unhealthy"
            self.circuit_breaker_open = True
            logger.error("Circuit breaker opened for vLLM")
        else:
            self.vllm_status = "degraded"

    def should_use_vllm(self):
        """判斷是否應該使用 vLLM"""
        if self.circuit_breaker_open:
            # 每 5 分鐘嘗試半開狀態
            if (datetime.now() - self.last_check).seconds > 300:
                self.circuit_breaker_open = False
                self.failure_count = 0
                return True
            return False
        return self.vllm_status in ["healthy", "degraded"]
```

#### 3.2.2 定期健康檢查

**背景任務** (使用 `asyncio`):

```python
async def health_check_loop():
    """每 30 秒檢查一次 vLLM 健康狀態"""
    while True:
        await health_monitor.check_vllm_health()
        await asyncio.sleep(30)

# 啟動時執行
asyncio.create_task(health_check_loop())
```

### 3.3 配置管理 API

#### 3.3.1 動態更新配置

**Endpoint**: `POST /api/v1/admin/llm/config/update`

```python
@app.post("/api/v1/admin/llm/config/update")
async def update_llm_config(request: Request):
    """動態更新 LLM Router 配置 (不需重啟)"""
    data = await request.json()

    # 驗證配置
    allowed_providers = ["openai", "vllm"]
    if data.get("low_provider") not in allowed_providers:
        return JSONResponse(
            {"error": "Invalid provider"},
            status_code=400
        )

    # 更新記憶體中的配置
    global llm_router_config
    llm_router_config["low"]["provider"] = data.get("low_provider")
    llm_router_config["low"]["model"] = data.get("low_model")

    # 持久化到 .env (可選)
    if data.get("persist", False):
        update_env_file({
            "LLM_LOW_PROVIDER": data["low_provider"],
            "LLM_LOW_MODEL": data["low_model"]
        })

    logger.info(f"LLM config updated: {data}")
    return JSONResponse({
        "status": "ok",
        "config": llm_router_config
    })
```

**使用範例**:
```bash
# 切換到 vLLM
curl -X POST http://localhost:8081/api/v1/admin/llm/config/update \
  -H "Content-Type: application/json" \
  -d '{
    "low_provider": "vllm",
    "low_model": "Qwen/Qwen2.5-7B-Instruct",
    "persist": true
  }'

# 緊急切換回 OpenAI (3090 失效時)
curl -X POST http://localhost:8081/api/v1/admin/llm/config/update \
  -H "Content-Type: application/json" \
  -d '{
    "low_provider": "openai",
    "low_model": "gpt-4.1-nano",
    "persist": false
  }'
```

#### 3.3.2 強制模式

**Endpoint**: `POST /api/v1/admin/llm/force-mode`

```python
@app.post("/api/v1/admin/llm/force-mode")
async def force_mode(request: Request):
    """強制使用特定模式 (覆蓋健康檢查)"""
    data = await request.json()
    mode = data.get("mode")  # all_openai | all_vllm | hybrid

    global forced_mode
    forced_mode = mode

    logger.warning(f"Forced mode activated: {mode}")
    return JSONResponse({"status": "ok", "mode": mode})
```

**使用場景**:
```bash
# 3090 維護時,強制全外部模式
curl -X POST http://localhost:8081/api/v1/admin/llm/force-mode \
  -d '{"mode": "all_openai"}'

# 恢復正常混合模式
curl -X POST http://localhost:8081/api/v1/admin/llm/force-mode \
  -d '{"mode": "hybrid"}'
```

### 3.4 路由決策邏輯

**改進的路由函數**:

```python
async def route_llm_request(tier: str, prompt: str):
    """
    智慧路由 LLM 請求

    Args:
        tier: "high" 或 "low"
        prompt: 使用者提示詞

    Returns:
        (provider, model, response)
    """
    # 1. 檢查強制模式
    if forced_mode == "all_openai":
        return await call_openai(prompt, "gpt-4.1-nano")

    # 2. HIGH tier 永遠用 OpenAI
    if tier == "high":
        return await call_openai(prompt, "gpt-4.1-nano")

    # 3. LOW tier 動態選擇
    if tier == "low":
        # 3.1 檢查 vLLM 健康狀態
        if health_monitor.should_use_vllm():
            try:
                response = await call_vllm(prompt, "Qwen/Qwen2.5-7B-Instruct")
                return ("vllm", "Qwen/Qwen2.5-7B-Instruct", response)
            except Exception as e:
                logger.error(f"vLLM call failed: {e}")
                health_monitor._handle_failure()
                # Fallback to OpenAI

        # 3.2 Fallback 或 Circuit breaker 開啟
        logger.info("Falling back to OpenAI for LOW tier")
        return await call_openai(prompt, "gpt-4.1-nano")
```

---

## 四、實作計畫

### Phase 1: 健康監控 (P0)

**目標**: 建立 3090 服務健康檢查機制

**檔案**: `proxy.py`

**實作項目**:
1. ✅ 新增 `HealthMonitor` 類別
2. ✅ 實作 `check_vllm_health()` 方法
3. ✅ 實作 Circuit Breaker 邏輯
4. ✅ 啟動背景健康檢查任務

**驗證**:
```bash
# 測試健康檢查端點
curl http://localhost:8081/api/v1/admin/llm/health

# 預期輸出
{
  "vllm": {
    "status": "healthy",
    "last_check": "2026-01-31T15:30:00",
    "circuit_breaker": false
  },
  "openai": {
    "status": "healthy"
  }
}
```

### Phase 2: 配置管理 API (P0)

**目標**: 允許動態切換 LLM 配置

**實作項目**:
1. ✅ `POST /api/v1/admin/llm/config/update` - 更新配置
2. ✅ `POST /api/v1/admin/llm/force-mode` - 強制模式
3. ✅ `POST /api/v1/admin/llm/reload` - 重載配置

**驗證**:
```bash
# 測試配置切換
curl -X POST http://localhost:8081/api/v1/admin/llm/config/update \
  -H "Content-Type: application/json" \
  -d '{"low_provider": "vllm", "low_model": "Qwen/Qwen2.5-7B-Instruct"}'
```

### Phase 3: 智慧路由 (P0)

**目標**: 根據健康狀態自動選擇 provider

**實作項目**:
1. ✅ 改進 `route_llm_request()` 函數
2. ✅ 整合 `HealthMonitor`
3. ✅ 實作自動 fallback 邏輯

**驗證**:
```bash
# 模擬 3090 失效
ssh ac-3090 "sudo systemctl stop vllm"

# 發送 LOW tier 請求,應自動 fallback 到 OpenAI
curl -X POST http://localhost:8081/api/v1/admin/llm/test \
  -d '{"tier": "low", "prompt": "Hello"}'

# 預期: provider = "openai" (自動 fallback)
```

### Phase 4: 監控與告警 (P1)

**目標**: 當服務狀態改變時發送通知

**實作項目**:
1. ⏸️ 整合 Telegram Bot 通知
2. ⏸️ 當 Circuit breaker 開啟時發送警報
3. ⏸️ 每日成本報告包含混合模式使用率

### Phase 5: 測試與文件 (P0)

**實作項目**:
1. ✅ 新增高可用性測試 (`test_high_availability.py`)
2. ✅ 失效場景測試
3. ✅ 更新知識庫文件

---

## 五、啟用真正混合模式步驟

### 5.1 前置檢查

```bash
# 1. 確認 vLLM 服務運行
ssh ac-3090 "curl -s http://localhost:8000/health | python3 -m json.tool"

# 2. 確認 SSH tunnel
ps aux | grep "ssh.*9000.*ac-3090"

# 3. 測試 vLLM 可用性
curl -s http://localhost:9000/v1/llm/generate \
  -H "Authorization: Bearer shc-compute-2026" \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Hello", "max_tokens": 10}'
```

### 5.2 修改配置

**在 acmacmini2 上執行**:

```bash
ssh acmacmini2

cd ~/workshop/super-happy-coder

# 備份原始配置
cp .env .env.backup.$(date +%Y%m%d_%H%M%S)

# 修改配置
cat >> .env << 'EOF'

# 啟用真正混合模式 (2026-01-31)
LLM_LOW_PROVIDER=vllm
LLM_LOW_MODEL=Qwen/Qwen2.5-7B-Instruct
EOF

# 驗證
grep "LLM_LOW" .env
```

### 5.3 重啟服務

```bash
# 找到 proxy.py 進程
ps aux | grep "proxy.py" | grep -v grep

# 優雅停止
pkill -f "proxy.py"

# 等待 2 秒
sleep 2

# 重新啟動
nohup python3 proxy.py > proxy.log 2>&1 &

# 檢查啟動狀態
tail -20 proxy.log
```

### 5.4 驗證配置

```bash
# 從 ac-mac 執行

# 1. 檢查配置
curl -s http://localhost:8081/api/v1/admin/llm/config | python3 -m json.tool

# 預期輸出:
# {
#   "low": {
#     "provider": "vllm",
#     "model": "Qwen/Qwen2.5-7B-Instruct"
#   }
# }

# 2. 測試 LOW tier 路由
curl -X POST http://localhost:8081/api/v1/admin/llm/test \
  -H "Content-Type: application/json" \
  -d '{
    "tier": "low",
    "prompt": "用繁體中文一句話說明 Python"
  }' | python3 -m json.tool

# 預期: "provider": "vllm"
```

---

## 六、運維指南

### 6.1 日常監控

**每日檢查**:
```bash
# 1. 健康狀態
curl http://localhost:8081/api/v1/admin/llm/health

# 2. 使用率統計
curl http://localhost:8081/api/v1/admin/stats

# 3. 錯誤率
grep "ERROR" ~/workshop/super-happy-coder/proxy.log | tail -20
```

### 6.2 故障處理

#### 場景 1: vLLM 服務停止

**症狀**:
- LOW tier 請求變慢或失敗
- 日誌出現 "vLLM health check failed"

**處理**:
```bash
# 1. 檢查 vLLM 狀態
ssh ac-3090 "systemctl status vllm"

# 2. 重啟 vLLM
ssh ac-3090 "sudo systemctl restart vllm"

# 3. 或臨時切換到全 OpenAI 模式
curl -X POST http://localhost:8081/api/v1/admin/llm/force-mode \
  -d '{"mode": "all_openai"}'
```

#### 場景 2: 3090 主機關機

**症狀**:
- 所有 Compute APIs 失效
- Circuit breaker 開啟

**處理**:
```bash
# 1. 確認 3090 狀態
ping -c 3 ac-3090

# 2. 強制全外部模式
curl -X POST http://localhost:8081/api/v1/admin/llm/force-mode \
  -d '{"mode": "all_openai"}'

# 3. 3090 恢復後解除強制模式
curl -X POST http://localhost:8081/api/v1/admin/llm/force-mode \
  -d '{"mode": "hybrid"}'
```

#### 場景 3: SSH Tunnel 斷線

**症狀**:
- "Connection refused" 錯誤

**處理**:
```bash
# ac-mac 上執行
~/super-happy-tests/ensure_tunnel.sh
```

### 6.3 成本追蹤

**每日報告**:
```bash
python3 /usr/local/bin/server-monitor/daily-claude-report.py
```

**預期輸出**:
```
=== Claude API 每日使用量報告 ===
日期: 2026-01-31

LLM Router 使用統計:
- HIGH tier (OpenAI):  30 requests, $0.90
- LOW tier (vLLM):     70 requests, $0.00
- Fallback 次數:        5 requests, $0.15

總成本: $1.05
節省: $1.95 (65%)
```

---

## 七、測試計畫

### 7.1 高可用性測試腳本

**檔案**: `/home/ac-mac/super-happy-tests/test_high_availability.py`

```python
"""
高可用性測試: 驗證動態切換與錯誤恢復能力
"""
import pytest
import requests
import time

BASE_URL = "http://localhost:8081"

class TestHighAvailability:

    def test_ha_01_health_check_endpoint(self):
        """驗證健康檢查端點"""
        resp = requests.get(f"{BASE_URL}/api/v1/admin/llm/health")
        assert resp.status_code == 200

        data = resp.json()
        assert "vllm" in data
        assert "openai" in data
        print(f"✅ Health status: {data}")

    def test_ha_02_vllm_to_openai_fallback(self):
        """測試 vLLM 失效時自動 fallback"""
        # 1. 正常情況: LOW tier 使用 vLLM
        resp = requests.post(
            f"{BASE_URL}/api/v1/admin/llm/test",
            json={"tier": "low", "prompt": "Hello"},
            timeout=30
        )
        assert resp.status_code == 200
        initial_provider = resp.json()["provider"]

        # 2. 模擬 vLLM 失效 (需要手動停止 vLLM 服務)
        # 這個測試需要人工干預,可以標記為 skip
        pytest.skip("需要手動停止 vLLM 服務來測試")

    def test_ha_03_dynamic_config_update(self):
        """測試動態配置更新"""
        # 1. 獲取目前配置
        resp = requests.get(f"{BASE_URL}/api/v1/admin/llm/config")
        original_config = resp.json()

        # 2. 切換到全 OpenAI
        resp = requests.post(
            f"{BASE_URL}/api/v1/admin/llm/config/update",
            json={
                "low_provider": "openai",
                "low_model": "gpt-4.1-nano",
                "persist": False
            }
        )
        assert resp.status_code == 200

        # 3. 驗證配置已更新
        resp = requests.get(f"{BASE_URL}/api/v1/admin/llm/config")
        new_config = resp.json()
        assert new_config["low"]["provider"] == "openai"

        # 4. 恢復原始配置
        requests.post(
            f"{BASE_URL}/api/v1/admin/llm/config/update",
            json={
                "low_provider": original_config["low"]["provider"],
                "low_model": original_config["low"]["model"],
                "persist": False
            }
        )
        print("✅ 動態配置更新成功")

    def test_ha_04_force_mode(self):
        """測試強制模式"""
        # 1. 強制全外部模式
        resp = requests.post(
            f"{BASE_URL}/api/v1/admin/llm/force-mode",
            json={"mode": "all_openai"}
        )
        assert resp.status_code == 200

        # 2. 發送 LOW tier 請求,應該用 OpenAI
        resp = requests.post(
            f"{BASE_URL}/api/v1/admin/llm/test",
            json={"tier": "low", "prompt": "Test"},
            timeout=30
        )
        assert resp.json()["provider"] == "openai"

        # 3. 恢復混合模式
        requests.post(
            f"{BASE_URL}/api/v1/admin/llm/force-mode",
            json={"mode": "hybrid"}
        )
        print("✅ 強制模式測試通過")
```

---

## 八、成本分析

### 8.1 假設情境

- 每天 100 個請求
- 30% 複雜任務 (HIGH tier)
- 70% 簡單任務 (LOW tier)
- 平均每請求 60 tokens

### 8.2 目前配置 (全 OpenAI)

```
HIGH tier: 30 × $0.03 = $0.90/天
LOW tier:  70 × $0.03 = $2.10/天
────────────────────────────────
總計:                  $3.00/天 = $90/月
```

### 8.3 混合模式 (OpenAI + vLLM)

```
HIGH tier: 30 × $0.03 = $0.90/天
LOW tier:  70 × $0.00 = $0.00/天 (vLLM 免費)
────────────────────────────────
總計:                  $0.90/天 = $27/月

節省: $63/月 (70%)
```

### 8.4 考慮 Fallback 的實際成本

假設 vLLM 可用率 95%:

```
HIGH tier: 30 × $0.03        = $0.90/天
LOW tier:
  - vLLM (95%):  66.5 × $0.00 = $0.00/天
  - Fallback (5%): 3.5 × $0.03 = $0.11/天
────────────────────────────────────────
總計:                          $1.01/天 = $30.3/月

節省: $59.7/月 (66%)
```

**結論**: 即使考慮 fallback,仍可節省約 66% 成本

---

## 九、總結

### 9.1 關鍵要點

1. **目前狀態**: LOW tier 使用 OpenAI,未啟用混合模式
2. **待實作**: 健康監控 + 配置管理 API + 智慧路由
3. **效益**: 節省 70% 成本,提高系統可靠性
4. **高可用**: Circuit breaker + 自動 fallback + 零停機配置

### 9.2 下一步

**立即執行** (P0):
1. ✅ 實作 `HealthMonitor` 類別
2. ✅ 新增配置管理 API
3. ✅ 改進路由邏輯
4. ✅ 建立高可用性測試

**後續優化** (P1):
1. ⏸️ Telegram 告警整合
2. ⏸️ 自動化成本報告
3. ⏸️ 效能監控儀表板

### 9.3 參考文件

- 測試報告: `/home/ac-mac/super-happy-tests/FINAL_REPORT.md`
- 混合模式配置: `/home/ac-mac/super-happy-tests/HYBRID_MODE_CONFIG.md`
- 工作日誌: `~/knowledge-base/work-logs/2026-01-31.md`

---

**文件版本**: v1.0
**建立日期**: 2026-01-31
**作者**: Alan Chen + Claude Code + Happy Engineering
**狀態**: 設計階段,待實作
