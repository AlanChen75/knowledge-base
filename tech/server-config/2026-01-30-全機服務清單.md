---
title: 全機服務清單
date: 2026-01-30
category: tech/server-config
tags: [服務清單, ac-mac, ac-3090, acmacmini2, systemd, 架構]
source: 工作日誌
---

# 全機服務清單

## 摘要
三台主機（Mac Mini、Mac Mini 2、3090）的完整服務清單與管理指令，截至 2026-01-30 最新狀態。

---

## 一、主機總覽

| 主機 | 別名 | Tailscale IP | 用途 |
|------|------|-------------|------|
| Mac Mini | ac-mac | 100.116.154.40 | 知識庫管理、TG Bot、監控中心 |
| Mac Mini 2 | acmacmini2 | 100.118.162.26 | Super Happy Coder Proxy |
| 3090 Server | ac-3090 | 100.108.119.78 | GPU 運算（LLM、Embedding、Rerank、OCR） |

---

## 二、3090 Server (ac-3090) 服務

### 硬體規格
- GPU: NVIDIA GeForce RTX 3090 (24GB VRAM)
- CPU: AMD Ryzen 9 3900X 12-Core
- RAM: 32GB
- NVIDIA Driver: 590.48.01 / CUDA Toolkit: 12.8
- PyTorch: 2.9.1+cu128 / flash-attn: 2.8.3

### systemd 服務

| 服務 | 端口 | 開機自啟 | 說明 |
|------|------|---------|------|
| **vllm.service** | 8000 (127.0.0.1) | ✅ | Qwen2.5-7B-Instruct 推理服務 |
| **compute-plane.service** | 9000 (0.0.0.0) | ✅ | Compute Plane API（統一 GPU 服務入口） |
| redis-server | 6379 | ✅ | 快取服務 |
| anydesk.service | 7070 | ✅ | 遠端桌面 |
| fail2ban.service | - | ✅ | SSH 安全防護 |
| nvidia-persistenced | - | ✅ | GPU 持久化服務 |
| sshd | 22 | ✅ | SSH 遠端連線 |

### vllm.service 詳細

```ini
[Unit]
Description=vLLM OpenAI API Server (Qwen2.5-7B-Instruct)
After=network.target

[Service]
User=ac3090
ExecStart=/usr/bin/python3 -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2.5-7B-Instruct \
    --host 127.0.0.1 --port 8000 \
    --max-model-len 4096 --gpu-memory-utilization 0.85 \
    --dtype float16 --enforce-eager
Restart=on-failure
RestartSec=10
Environment=CUDA_HOME=/usr/local/cuda-12.8
Environment=DISABLE_MODEL_SOURCE_CHECK=True
```

管理指令：
```bash
sudo systemctl status vllm
sudo systemctl restart vllm
sudo systemctl stop vllm
journalctl -u vllm -f
```

### compute-plane.service 詳細

- 程式碼：`/home/ac3090/compute-plane/main.py`
- API Key：`shc-compute-2026`（Bearer Token）
- 轉發 LLM 請求到 vLLM (localhost:8000)

API 端點：
```
GET  /health                    # 健康檢查（不需認證）
GET  /v1/gpu/status             # GPU 狀態
POST /v1/llm/generate           # LLM 生成（→ vLLM）
POST /v1/llm/tool-call          # LLM 工具呼叫（→ vLLM）
POST /v1/embeddings             # 文字向量（bge-base-zh-v1.5, 768維）
POST /v1/rerank                 # 文件重排序（bge-reranker-v2-m3）
POST /v1/ocr/submit             # OCR 提交（非同步）⚠️ use_gpu 參數待修
GET  /v1/ocr/result/{job_id}    # OCR 結果查詢
POST /v1/tools/run              # 工具執行（ruff/pytest/mypy/eslint）
```

管理指令：
```bash
sudo systemctl status compute-plane
sudo systemctl restart compute-plane
journalctl -u compute-plane -f
```

### GPU 記憶體分配（正常運行時）

| 進程 | 佔用 |
|------|------|
| vLLM (Qwen2.5-7B FP16) | ~21 GB |
| Xorg + gnome-shell | ~50 MB |
| Embedding/Rerank（延遲載入） | ~1-2 GB |
| **合計** | ~22-23 GB / 24 GB |

### 已停用服務
- **ComfyUI**：手動停用，路徑 `/home/ac3090/ComfyUI/`，原佔用 ~256MB VRAM

---

## 三、Mac Mini 2 (acmacmini2) 服務

### systemd 服務

| 服務 | 端口 | 開機自啟 | 說明 |
|------|------|---------|------|
| **super-happy-coder.service** | 8081 | ✅ | SHC Proxy（Flask） |
| **ssh-tunnel-3090.service** | 9000 (轉發) | ✅ | SSH Tunnel 到 3090:9000 |

### super-happy-coder.service

- 程式碼：`/home/ac-macmini2/workshop/super-happy-coder/`
- CLI Backend：**Claude**（`Environment=CLI_BACKEND=claude`）
- Agent IDs：M1 (CLI Agent), M2 (Web Deploy), M3 (RAG)

管理指令：
```bash
sudo systemctl status super-happy-coder
sudo systemctl restart super-happy-coder
journalctl -u super-happy-coder -f
```

### ssh-tunnel-3090.service

- 將 acmacmini2:9000 轉發到 ac-3090:9000
- 讓 Super Happy Coder 可透過 localhost:9000 存取 Compute Plane

---

## 四、Mac Mini (ac-mac) 服務

### 系統服務

| 服務 | 說明 |
|------|------|
| **tg-monitor-bot.service** | @ac_server_monitor_bot — 系統監控 TG Bot |
| **tg-claude-bot** (手動) | @RemoteAi123_bot — 知識庫助手 TG Bot |

### Telegram Bots

| Bot | Username | 用途 | 管理方式 |
|-----|----------|------|---------|
| Server Monitor | @ac_server_monitor_bot | 系統監控、SSH 通知 | systemd |
| 知識庫助手 | @RemoteAi123_bot | 知識庫管理、Claude | 手動 nohup |
| ComfyUI Bot | @ac_comfyui_bot | 圖片生成 | 手動（已停用） |

### 知識庫
- 路徑：`/home/ac-mac/knowledge-base/`
- Git 管理：✅

---

## 五、跨機架構圖

```
用戶 (Telegram / API)
       │
       ▼
┌─────────────────────┐
│  Mac Mini 2         │
│  (acmacmini2)       │
│                     │
│  super-happy-coder  │──── CLI Backend: Claude
│  :8081              │
│                     │
│  ssh-tunnel-3090    │
│  :9000 ─────────────┼──── SSH Tunnel ────┐
└─────────────────────┘                    │
                                           ▼
┌──────────────────────────────────────────────┐
│  3090 Server (ac-3090)                       │
│                                              │
│  compute-plane :9000                         │
│  ├─ /v1/llm/*      ──→  vllm :8000          │
│  │                      (Qwen2.5-7B-Instruct)│
│  ├─ /v1/embeddings  ──→  bge-base-zh-v1.5   │
│  ├─ /v1/rerank      ──→  bge-reranker-v2-m3 │
│  ├─ /v1/ocr/*       ──→  PaddleOCR          │
│  └─ /v1/tools/*     ──→  ruff/pytest/eslint  │
│                                              │
│  GPU: RTX 3090 24GB                          │
└──────────────────────────────────────────────┘

┌─────────────────────┐
│  Mac Mini (ac-mac)  │
│                     │
│  tg-monitor-bot     │──── @ac_server_monitor_bot
│  tg-claude-bot      │──── @RemoteAi123_bot
│  knowledge-base/    │──── 知識庫 (Git)
└─────────────────────┘
```

---

## 六、串接測試結果（2026-01-30）

### Compute Plane → vLLM 本機串接

| 測試 | 結果 | 備註 |
|------|------|------|
| Health Check | ✅ | 無需認證 |
| GPU 狀態 | ✅ | 44°C, 15W (待機) |
| LLM 生成 | ✅ | 1.2s, 繁體中文正常 |
| LLM Tool Call | ✅ | 0.2s |
| Embedding | ✅ | 768 維, bge-base-zh-v1.5 |
| Rerank | ✅ | 排序正確，相關度 0.998 |
| Toolchain (lint) | ✅ | ruff 正確偵測未使用 import |
| OCR | ✅ | PaddleOCR 3.3.3 + PaddlePaddle GPU 3.3.0 (cu129) |

---

## 七、已知問題

1. ~~**OCR use_gpu 參數**~~：已修復，升級至 PaddlePaddle GPU 3.3.0 (cu129) + PaddleOCR 3.3.3
2. **Tailscale ACL 不同步**：acmacmini2 → ac-3090 直連 port 9000 受阻，目前靠 SSH Tunnel 繞過
3. **vLLM 進程殘留**：啟動失敗後 EngineCore 可能殘留佔用 GPU 記憶體，需手動 kill
4. **paddlex 插件警告**：`Failed to load plugin register_paddlex_genai_models`，不影響運行
5. **CUDA libs 版本差異**：PaddlePaddle GPU 3.3.0 (cu129) 安裝了 12.9 版 nvidia-* libs，PyTorch 2.9.1 需要 12.8 版，但 12.9 向下相容，實測 PyTorch/vLLM/PaddleOCR 均正常運作

---

*最後更新: 2026-01-30*
