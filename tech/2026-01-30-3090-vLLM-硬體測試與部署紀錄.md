---
title: 3090 vLLM 硬體測試與部署紀錄
date: 2026-01-30
category: tech
tags: [3090, vLLM, GPU, flash-attn, Qwen2.5, 硬體測試]
source: 本地測試
---

# 3090 vLLM 硬體測試與部署紀錄

## 摘要
完成 RTX 3090 硬體壓力測試（6/6 通過），安裝 CUDA 12.8 toolkit + flash-attn 2.8.3，成功啟動 vLLM 0.14.1 並完成 Qwen2.5-7B-Instruct 推理測試。

## 系統環境

| 項目 | 規格 |
|------|------|
| GPU | NVIDIA GeForce RTX 3090 (24GB) |
| CPU | AMD Ryzen 9 3900X 12-Core |
| RAM | 32GB |
| OS | Ubuntu 22.04 (Kernel 6.8.0-90-generic) |
| NVIDIA Driver | 590.48.01 |
| CUDA | 13.1 (Driver) / 12.8 (Toolkit) |
| PyTorch | 2.9.1+cu128 |
| vLLM | 0.14.1 |
| flash-attn | 2.8.3 |
| flashinfer | 0.5.3 |
| triton | 3.5.1 |

## 一、硬體壓力測試結果（6/6 通過）

| 測試項目 | 結果 | 詳細 |
|---------|------|------|
| 基本 CUDA 運算 | 通過 | 矩陣乘法正常 |
| 記憶體分配/釋放 | 通過 | 最大成功分配 20GB |
| 大規模矩陣運算 | 通過 | 68.76 TFLOPS (FP16, 8192x8192) |
| 記憶體完整性 | 通過 | 5 種模式寫入/讀回驗證無誤 |
| 混合精度運算 | 通過 | FP32/FP16/BF16 全部正常 |
| 持續高負載 60s | 通過 | 29689 次迭代, 65.63 TFLOPS, 0 錯誤 |

壓力測試期間 GPU 溫度從 43°C 升至 79°C，功耗 ~344W（上限 350W），表現穩定。

## 二、vLLM 啟動問題排查歷程

### 問題描述
vLLM 0.14.1 (V1 引擎) 無法啟動，嘗試多種 attention 後端均失敗。

### 嘗試紀錄

| 嘗試 | 後端 | 結果 | 原因 |
|------|------|------|------|
| 1 | FLASH_ATTN (預設) | 掛起 | flash-attn 未安裝 |
| 2 | FLASH_ATTN + --enforce-eager | TimeoutError | 同上 |
| 3 | XFORMERS | 啟動失敗 | 不是 vLLM 0.14.1 有效選項 |
| 4 | TORCH_SDPA | 啟動失敗 | "must be registered before use" |
| 5 | FLASHINFER | 啟動失敗 | Engine core initialization failed |

### 根本原因
- vLLM 0.14.1 的 V1 引擎預設使用 FLASH_ATTN 後端
- `flash-attn` 套件未安裝，且需要 nvcc 從原始碼編譯
- 系統只有 NVIDIA Driver，沒有 CUDA Toolkit

### 解決步驟
1. `sudo apt-get install nvidia-cuda-toolkit` (CUDA 11.5，不夠新)
2. 添加 CUDA 12 倉庫 + `sudo apt-get install cuda-toolkit-12-8`
3. 設定 `CUDA_HOME=/usr/local/cuda-12.8`
4. `pip3 install flash-attn --no-build-isolation`（編譯耗時 ~60 分鐘）
5. flash-attn 2.8.3 安裝成功

## 三、vLLM 推理測試結果

### 啟動參數
```bash
python3 -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2.5-7B-Instruct \
    --host 127.0.0.1 \
    --port 8000 \
    --max-model-len 4096 \
    --gpu-memory-utilization 0.85 \
    --dtype float16 \
    --enforce-eager
```

### 測試結果

| 測試 | 結果 | 回應時間 |
|------|------|---------|
| Health Check | 通過 | - |
| 模型列表 (/v1/models) | 通過 | - |
| 中文生成 | 通過 | 1.15s (52 tokens) |
| 英文生成 | 通過 | 0.79s (39 tokens) |
| 程式碼生成 | 通過 | 5.99s (300 tokens) |

### 生成品質範例
**中文**: 「機器學習是一種人工智能技術，讓電腦能通過資料分析和模型構建來自我學習，而不需要明確程式編寫。簡單來說，就是讓電腦從大量數據中學習規律並做出預測或決策。」

### GPU 使用
- 推理時 GPU 記憶體: 21257 MiB / 24576 MiB (86.5%)
- 推理時 GPU 溫度: 66°C
- 推理時功耗: 347.74W

## 四、注意事項

1. **paddlex 插件警告**: `Failed to load plugin register_paddlex_genai_models`，不影響 vLLM 運行
2. **ComfyUI 佔用 256MiB GPU**: 開機自動啟動，不影響 vLLM
3. **vLLM 進程殘留**: 啟動失敗後可能殘留 EngineCore 進程佔用 GPU 記憶體，需手動 kill
4. **flash-attn 編譯**: 72 個 CUDA kernel 文件 × 4 架構 = 大量編譯時間

## 五、服務管理

```bash
# 啟動 vLLM
export CUDA_HOME=/usr/local/cuda-12.8
export DISABLE_MODEL_SOURCE_CHECK=True
nohup python3 -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2.5-7B-Instruct \
    --host 127.0.0.1 --port 8000 \
    --max-model-len 4096 --gpu-memory-utilization 0.85 \
    --dtype float16 --enforce-eager \
    > /tmp/vllm.log 2>&1 &

# 檢查狀態
curl http://127.0.0.1:8000/health
curl http://127.0.0.1:8000/v1/models

# 停止 vLLM
pkill -f "vllm.entrypoints"
```
