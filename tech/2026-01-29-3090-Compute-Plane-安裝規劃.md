---
title: 3090 Compute Plane 安裝規劃
date: 2026-01-29
category: tech
tags: [3090, GPU, compute-plane, super-happy-coder, deployment]
source: OpenSpec v3 specs/56-3090-compute-plane-deployment.md
---

# 3090 Compute Plane 安裝規劃

## 摘要

規劃在 RTX 3090 主機 (ac-3090) 上安裝 Super Happy Coder 所需的 Compute Plane 服務。
目標：提供 LLM 推理、Embedding、Rerank、OCR、Toolchain 五大服務，
由 Mac Mini 2 上的 Agent Executor 透過內網 API 呼叫。

---

## 現況評估

### 硬體
| 項目 | 規格 |
|------|------|
| GPU | NVIDIA RTX 3090 24GB VRAM |
| RAM | 32GB |
| 磁碟 | 457GB (394GB 可用) |
| OS | Ubuntu 22.04 LTS |
| NVIDIA Driver | 590.48.01 |
| CUDA (PyTorch) | 12.1 |

### 已安裝
- Python 3 + pip3
- PyTorch 2.5.1+cu121（CUDA 可用）
- Git
- ComfyUI（使用中，佔用約 7GB VRAM）

### 未安裝（需安裝）
- ❌ CUDA Toolkit (nvcc) — 編譯用
- ❌ Docker — 容器化部署（可選，MVP 先不裝）
- ❌ FastAPI + Uvicorn — API 服務框架
- ❌ Redis — Job Queue / Cache
- ❌ vLLM 或 llama.cpp — LLM 推理引擎
- ❌ sentence-transformers — Embedding 服務
- ❌ faiss-gpu — 向量搜尋（可選）
- ❌ PaddleOCR + PaddlePaddle — OCR 服務
- ❌ poppler-utils — PDF 前處理
- ❌ Node.js — Toolchain (M2 Web Deploy)

### VRAM 分配考量
- 總 VRAM：24GB
- ComfyUI 佔用：~7GB
- 可用 VRAM：~17GB
- 若需全量部署 LLM (7B~13B)，可能需要暫停 ComfyUI

---

## 安裝計畫

### Phase 1：基礎環境（必裝）

```bash
# 1. 系統套件更新
sudo apt update && sudo apt upgrade -y

# 2. FastAPI + Uvicorn（API 框架）
pip3 install fastapi uvicorn[standard] python-multipart aiofiles

# 3. Redis（Job Queue / Cache）
sudo apt install -y redis-server
sudo systemctl enable redis-server
sudo systemctl start redis-server
pip3 install redis

# 4. 常用工具
pip3 install httpx pydantic pyyaml python-jose[cryptography]

# 5. poppler-utils（PDF 處理）
sudo apt install -y poppler-utils imagemagick
```

### Phase 2：LLM 推理引擎（擇一）

#### 方案 A：vLLM（推薦，吞吐量高）
```bash
pip3 install vllm

# 啟動 vLLM server（範例：Qwen2.5-7B）
python3 -m vllm.entrypoints.openai.api_server \
  --model Qwen/Qwen2.5-7B-Instruct \
  --host 0.0.0.0 --port 8000 \
  --max-model-len 4096 \
  --gpu-memory-utilization 0.5
```

#### 方案 B：llama.cpp（量化模型，省 VRAM）
```bash
# 編譯 llama.cpp（需 CUDA toolkit）
sudo apt install -y nvidia-cuda-toolkit
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp && make LLAMA_CUDA=1 -j$(nproc)

# 啟動 server
./llama-server -m models/qwen2.5-7b-q4_k_m.gguf \
  --host 0.0.0.0 --port 8000 -ngl 99
```

#### VRAM 預估
| 模型 | 精度 | VRAM | 與 ComfyUI 共存 |
|------|------|------|-----------------|
| Qwen2.5-7B | FP16 | ~14GB | ❌ 需暫停 ComfyUI |
| Qwen2.5-7B | Q4_K_M | ~5GB | ✅ 可共存 |
| Qwen2.5-3B | FP16 | ~7GB | ✅ 可共存 |
| Qwen2.5-3B | Q4_K_M | ~2.5GB | ✅ 可共存 |

### Phase 3：Embedding + Rerank

```bash
# sentence-transformers（Embedding）
pip3 install sentence-transformers

# 推薦模型（中文效果好）
# - BAAI/bge-base-zh-v1.5（~400MB，中文優化）
# - BAAI/bge-m3（多語言，較大）

# Rerank 模型
# - BAAI/bge-reranker-v2-m3（多語言 reranker）

# faiss-gpu（可選，本機向量索引）
pip3 install faiss-gpu
```

### Phase 4：OCR 服務

```bash
# PaddlePaddle GPU 版
pip3 install paddlepaddle-gpu

# PaddleOCR
pip3 install paddleocr

# 測試
python3 -c "from paddleocr import PaddleOCR; ocr = PaddleOCR(use_angle_cls=True, lang='ch'); print('OK')"
```

### Phase 5：Toolchain

```bash
# Node.js (LTS)
curl -fsSL https://deb.nodesource.com/setup_20.x | sudo -E bash -
sudo apt install -y nodejs

# Python 工具
pip3 install ruff mypy pytest

# 驗證
node --version && npm --version && ruff --version
```

---

## 統一 API 服務架構

MVP 階段先用單一 FastAPI 整合所有路由，後續再拆分：

```
ac-3090:9000/
├── GET  /health                    # 健康檢查
├── POST /v1/llm/generate           # LLM 生成
├── POST /v1/llm/tool-call          # LLM 工具呼叫
├── POST /v1/embeddings             # Embedding
├── POST /v1/rerank                 # Rerank
├── POST /v1/ocr/submit             # OCR 提交（非同步）
├── GET  /v1/ocr/result/{job_id}    # OCR 結果查詢
└── POST /v1/tools/run              # 工具執行
```

### 安全設計
- 僅監聽內網 IP（Tailscale: 100.x.x.x）
- API Key 認證（簡單 Bearer Token）
- Request ID 追蹤
- Timeout / Size Limit

### 服務管理
```bash
# systemd 服務（建議）
# /etc/systemd/system/compute-plane.service
[Unit]
Description=Super Happy Coder Compute Plane
After=network.target redis-server.service

[Service]
User=ac3090
WorkingDirectory=/home/ac3090/compute-plane
ExecStart=/usr/bin/python3 -m uvicorn main:app --host 0.0.0.0 --port 9000
Restart=always
Environment=CUDA_VISIBLE_DEVICES=0

[Install]
WantedBy=multi-user.target
```

---

## 安裝優先順序

| 順序 | 項目 | 理由 |
|------|------|------|
| 1 | 基礎環境 (FastAPI/Redis) | 所有服務的基礎 |
| 2 | Embedding + Rerank | VRAM 用量少，可與 ComfyUI 共存 |
| 3 | OCR (PaddleOCR) | 中文場景常用 |
| 4 | LLM 推理 (量化版) | 核心功能，但需決定 VRAM 策略 |
| 5 | Toolchain (Node.js) | 對應 M2 Web Deploy |

---

## 待決策事項

1. **ComfyUI 共存策略**：是否需要 ComfyUI 持續運行？
   - 若需要 → LLM 只能用量化模型 (Q4)，可用 VRAM ~17GB
   - 若可暫停 → LLM 可用 FP16 (7B)，效果更好
2. **LLM 模型選擇**：Qwen2.5-7B vs 3B vs 其他
3. **推理引擎選擇**：vLLM vs llama.cpp
4. **是否使用 Docker**：MVP 先直接裝，後續再容器化？
