---
title: 3090 遠端壓力測試報告（20/30/50 學生）
date: 2026-01-30
category: tech
tags: [3090, vLLM, stress-test, Qwen2.5, Compute-Plane]
source: 本機測試
---

# 3090 遠端壓力測試報告

## 摘要

從 ac-mac 透過 SSH Tunnel 對 3090 上的 vLLM (Qwen2.5-7B-Instruct) 進行遠端壓力測試。分別測試 vLLM 直連 (port 8000) 與 Compute Plane API (port 9000) 兩個端點，模擬 20/30/50 學生同時發送交叉複雜任務。**全部測試 100% 成功，50 人同時並發也在 28 秒內完成。**

## 關鍵要點

- 50 學生同時並發，成功率 100%，無任何超時或錯誤
- vLLM continuous batching 機制極為高效，20→50 人批次總耗時僅增加 4 秒
- Compute Plane proxy 層額外開銷在 5-8% 之間，可忽略
- 網路頻寬完全不是瓶頸（Tailscale 延遲 ~5ms，LLM API 純文字傳輸量極小）
- GPU 推理是唯一瓶頸：排隊越多，個別請求等待時間越長

---

## 1. 測試環境

| 項目 | 規格 |
|------|------|
| **測試端** | ac-mac (Mac Mini)，透過 SSH Tunnel |
| **目標端** | 3090 (RTX 3090 24GB, Ryzen 9 3900X, 32GB RAM) |
| **模型** | Qwen/Qwen2.5-7B-Instruct (vLLM v0.14.1) |
| **vLLM 設定** | max-model-len 4096, gpu-memory-utilization 0.75, dtype float16, enforce-eager |
| **網路** | Tailscale 內網 + SSH Tunnel，延遲 ~5ms |
| **測試工具** | stress_test_3090_dual.py (aiohttp 非同步並發) |

### 測試端點
- **vLLM 直連**: `localhost:8000/v1/chat/completions` (OpenAI-compatible API)
- **Compute Plane**: `localhost:9000/v1/llm/generate` (FastAPI proxy → vLLM)

---

## 2. 任務設計

模擬真實課堂場景，6 種任務類型交叉分配：

| 任務類型 | 題數 | 說明 | max_tokens |
|----------|:----:|------|:----------:|
| code (程式碼生成) | 15 | BST、HTTP 伺服器、爬蟲框架等 | 768-1024 |
| explain (概念解釋) | 10 | GIL、RESTful、微服務、SOLID 等 | 768 |
| debug (除錯修復) | 8 | merge sort、deadlock、SQL injection 等 | 512 |
| design (系統設計) | 7 | 即時通訊、短網址、搶票系統等 | 1024 |
| algorithm (演算法) | 5 | Dijkstra、背包、A*、紅黑樹等 | 768-1024 |
| long (長文生成) | 5 | README、CI/CD 教學、面試指南等 | 1024 |

---

## 3. 規模遞增測試結果

### 3.1 總覽比較

#### vLLM 直連 (port 8000)

| 指標 | 20 學生 | 30 學生 | 50 學生 |
|------|:-------:|:-------:|:-------:|
| 成功率 | 20/20 (100%) | 30/30 (100%) | 50/50 (100%) |
| 批次總耗時 | 22.3s | 26.2s | 26.6s |
| 平均回應時間 | 15.5s | 18.9s | 18.9s |
| 最短回應 | 11.6s | 11.4s | 10.0s |
| 最長回應 | 22.3s | 26.2s | 26.5s |
| 總 tokens | 15,135 | 22,798 | 38,042 |
| 平均 tokens/請求 | 756 | 759 | 760 |

#### Compute Plane (port 9000)

| 指標 | 20 學生 | 30 學生 | 50 學生 |
|------|:-------:|:-------:|:-------:|
| 成功率 | 20/20 (100%) | 30/30 (100%) | 50/50 (100%) |
| 批次總耗時 | 23.1s | 24.9s | 27.5s |
| 平均回應時間 | 16.8s | 17.0s | 20.0s |
| 最短回應 | 12.7s | 8.2s | 10.0s |
| 最長回應 | 23.1s | 24.8s | 27.5s |
| 總 tokens | 15,513 | 22,024 | 38,447 |
| 平均 tokens/請求 | 775 | 734 | 768 |

### 3.2 Compute Plane 額外延遲

| 規模 | 額外延遲 |
|:----:|:--------:|
| 20 人 | +8.1% |
| 30 人 | -9.7%（快取效應） |
| 50 人 | +5.5% |

結論：proxy 層開銷極小，在誤差範圍內。

### 3.3 按任務類型分析（50 學生 vLLM 直連）

| 任務類型 | 平均回應時間 | 平均 tokens |
|----------|:-----------:|:-----------:|
| debug | 13.3s | 593 |
| explain | 18.6s | 724 |
| code | 19.9s | 781 |
| design | 19.5s | 775 |
| algorithm | 21.8s | 877 |
| long | 22.2s | 900 |

---

## 4. 頻寬分析

### 實測網路數據

| 項目 | 數值 |
|------|------|
| Tailscale ping 延遲 | 2-9ms (平均 4.8ms) |
| 50 人同時請求上行 | ~60 KB（瞬間） |
| 50 人回應下行 | ~1.5 MB（分佈在 27 秒內） |
| 結論 | **頻寬完全不是瓶頸** |

### 從 ac-mac 遠端測試的優勢
1. 模擬真實使用場景（學生也是透過網路呼叫）
2. 端到端時間包含：網路延遲 + 排隊 + 推理 + 回傳
3. 驗證 SSH Tunnel 穩定性

---

## 5. GPU 資源觀察

### 壓力測試前
- 溫度：50°C
- 功耗：28.6W（待機）
- VRAM：1,864 MB / 24,124 MB

### 壓力測試中（推估）
- vLLM 推理時 VRAM：~18.5 GB
- GPU 使用率：接近 100%
- vLLM KV cache prefix hit rate：55.5%（有效快取重複前綴）

### 關鍵觀察
- vLLM continuous batching 讓 20→50 人的**批次總耗時**幾乎不變（22s→27s）
- 代價是個別請求的**等待時間**增加（15s→19s 平均）
- 生成吞吐量穩定在 ~97 tokens/s

---

## 6. 結論與建議

### 6.1 容量結論

| 場景 | 能否支撐 | 備註 |
|------|:--------:|------|
| 20 學生課堂 | ✅ 完全可以 | 平均 15s 回應 |
| 30 學生課堂 | ✅ 可以 | 平均 19s 回應 |
| 50 學生課堂 | ✅ 可以但體驗下降 | 平均 19-20s，最長 27s |
| >50 學生 | ⚠️ 未測試 | 預期回應時間會更長 |

### 6.2 建議

1. **20-30 學生**的課堂：直接使用，體驗良好
2. **30-50 學生**：可用，但建議分批發送或設定排隊機制
3. **Compute Plane vs vLLM 直連**：效能幾乎一樣，建議使用 Compute Plane（有認證、統一入口）
4. **如需更好體驗**：可考慮降低 max_tokens 或使用量化模型（Q4/Q8）提升吞吐量

---

## 7. 測試腳本

```bash
# 測試工具位置
~/workshop/tools/stress_test_3090_dual.py

# 用法
python3 stress_test_3090_dual.py quick       # 快速驗證（3 學生）
python3 stress_test_3090_dual.py dual 20     # 雙端點 20 學生
python3 stress_test_3090_dual.py dual 30     # 雙端點 30 學生
python3 stress_test_3090_dual.py dual 50     # 雙端點 50 學生
python3 stress_test_3090_dual.py scale       # 遞增測試（20→30→50）
python3 stress_test_3090_dual.py vllm 50     # 只測 vLLM 50 學生
python3 stress_test_3090_dual.py compute 50  # 只測 Compute Plane 50 學生
```

### JSON 報告檔案
- `stress_test_3090_20s_report_20260130_*.json`
- `stress_test_3090_30s_report_20260130_*.json`
- `stress_test_3090_50s_report_20260130_*.json`
- `stress_test_3090_scale_report_20260130_*.json`

---

## 8. 其他 API 測試結果

除了 LLM 推理外，Compute Plane 上的其他 API 也從 ac-mac 遠端驗證通過：

### 8.1 Embedding API (`/v1/embeddings`)

| 項目 | 結果 |
|------|------|
| 狀態 | ✅ 正常 |
| 模型 | BAAI/bge-base-zh-v1.5 |
| 維度 | 768 |
| 測試 | 3 段中文文字，正確回傳 3 個 embedding 向量 |

```bash
curl -X POST http://localhost:9000/v1/embeddings \
  -H "Authorization: Bearer shc-compute-2026" \
  -d '{"texts":["你好世界","Python 程式設計","機器學習入門"]}'
```

### 8.2 Rerank API (`/v1/rerank`)

| 項目 | 結果 |
|------|------|
| 狀態 | ✅ 正常 |
| 模型 | BAAI/bge-reranker-v2-m3 |
| 測試 | query="如何學習 Python"，5 篇文件重排序 |
| 結果 | "Python 入門教學指南" 排第一 (score: 0.773)，正確 |

```bash
curl -X POST http://localhost:9000/v1/rerank \
  -H "Authorization: Bearer shc-compute-2026" \
  -d '{"query":"如何學習 Python","documents":["Python 是一種程式語言","今天天氣很好","Python 入門教學指南"],"top_k":3}'
```

### 8.3 OCR API (`/v1/ocr/submit` + `/v1/ocr/result/{job_id}`)

| 項目 | 結果 |
|------|------|
| 狀態 | ✅ 正常（非同步處理） |
| 引擎 | Surya OCR |
| 注意 | 不支援 SVG 格式，需要 PNG/JPG |

流程：submit → 取得 job_id → poll result

#### 測試一：簡單文字

| 項目 | 結果 |
|------|------|
| 測試內容 | PIL 生成含 "Hello Python 3090 Test" 的簡單圖片 |
| 平均信心度 | 0.82 |
| 結論 | 可辨識，但低品質圖片（PIL 預設字型）信心度偏低 |

#### 測試二：A4 文件（中英文混合 + 數據表格）

| 項目 | 結果 |
|------|------|
| 測試內容 | A4 尺寸 (1240×1754 @150DPI) 模擬業績報告 |
| 內容 | 中文標題、英文段落、財務數據表格（NT$ 金額）、百分比、技術指標 |
| 辨識區塊數 | 78 個 |
| 平均信心度 | **0.97** |
| 處理速度 | <3 秒 |

按類型信心度分析：

| 內容類型 | 平均信心度 | 說明 |
|----------|:---------:|------|
| 中文標題 | 0.997 | 「2026年度第一季度業績報告」 |
| 英文段落 | 0.999 | "The company achieved revenue..." |
| 中文段落 | 0.995 | 「本季度公司營收達到新台幣 3.28 億元...」 |
| 金額數據 | 0.983 | NT$328,000,000 等全部正確 |
| 百分比 | 0.957 | +15.7%, +1.2pp 等全部正確 |
| 中英混合欄位 | 0.965 | 「毛利率 (Gross Margin)」、「EPS (每股盈餘)」 |
| 技術術語 | 0.975 | Qwen2.5-7B-Instruct、P95: 345ms、tokens/s |
| 頁尾小字 | 0.970 | 「機密文件 Confidential」、「第 1 頁，共 3 頁」 |

結論：**高品質文件圖片的 OCR 辨識極為準確，中英文混合與數據表格全部正確辨識。信心度從簡單測試的 0.82 提升至 0.97。**

### 8.4 Toolchain API (`/v1/tools/run`)

| 工具 | 語言 | 狀態 | 說明 |
|------|------|:----:|------|
| `lint` | python | ✅ | 使用 ruff/mypy 檢查程式碼 |
| `test` | python | ✅ | 使用 pytest 執行測試 |

```bash
# Lint
curl -X POST http://localhost:9000/v1/tools/run \
  -H "Authorization: Bearer shc-compute-2026" \
  -d '{"tool":"lint","language":"python","code":"def foo(): return 1"}'

# Test
curl -X POST http://localhost:9000/v1/tools/run \
  -H "Authorization: Bearer shc-compute-2026" \
  -d '{"tool":"test","language":"python","code":"def test_add(): assert 1+1==2"}'
```

### 8.5 API 完整狀態總覽

| API | 端點 | 狀態 | 備註 |
|-----|------|:----:|------|
| LLM 生成 | `/v1/llm/generate` | ✅ | 50 人並發測試通過 |
| Tool Call | `/v1/llm/tool-call` | ✅ | （先前測試通過） |
| Embedding | `/v1/embeddings` | ✅ | 768 維向量 |
| Rerank | `/v1/rerank` | ✅ | 重排序正確 |
| OCR | `/v1/ocr/submit` | ✅ | 非同步，需 poll |
| Toolchain | `/v1/tools/run` | ✅ | lint + test |
| GPU 狀態 | `/v1/gpu/status` | ✅ | 溫度、記憶體、使用率 |
| 健康檢查 | `/health` | ✅ | 含 GPU + Redis 狀態 |

---

**測試時間**: 2026-01-30 21:38 - 22:10 (UTC+8)
**測試執行者**: Claude Code (from ac-mac)
