
## [10:48] SHC M6 簡報生成系統 - GPT-5 mini 支援
- **來源**: Happy Coder 接續任務
- **Session**: c44476a7
- **狀態**: 進行中
- **目標**: 修改 SHC 程式碼支援 OpenAI GPT-5 系列 API

### 背景
測試 SHC M6 簡報生成系統時發現：
- GPT-4.1-nano 可正常使用（透過 SHC Proxy）
- GPT-5 mini 無法使用（API 參數不相容）
- 錯誤訊息：`Unsupported parameter: 'max_tokens'`

### 根本原因分析
OpenAI 在 GPT-5 系列變更 API 規格：
1. **Token 限制參數**：
   - GPT-4.x: 使用 `max_tokens`
   - GPT-5.x: 使用 `max_completion_tokens`（不支援舊參數）

2. **Temperature 參數**：
   - GPT-4.x: 0.0-2.0 可調
   - GPT-5.x: 固定 1.0（不可設定）

3. **SHC 問題位置**：
   - 檔案：`llm_router.py` 的 `OpenAIAdapter.chat()` 方法
   - 寫死使用舊版參數 `max_tokens` 和 `temperature`

### 已完成
- [10:48] 詳細分析問題根因
- [10:50] 記錄到知識庫：`~/knowledge-base/tech/devops/shc-openai-api-compatibility.md`
- [10:52] 撰寫工作日誌

### 下一步
1. 備份原始 `llm_router.py`
2. 修改 `OpenAIAdapter.chat()` 方法支援 GPT-5 API
3. 測試修改後能否正常呼叫 GPT-5 mini
4. 重啟 SHC 服務並驗證
5. 使用 SHC Proxy 生成測試簡報比較品質

### 技術決策
採用**動態參數選擇**方案：
- 根據模型名稱前綴 (`gpt-5`) 判斷使用新舊參數
- 向下相容既有 GPT-4.x 模型
- 最小化程式碼變更風險


### [11:23] GPT-5 mini 問題解決
- **問題根因**：GPT-5 mini 的 reasoning tokens 耗盡 `max_completion_tokens` 配額
- **症狀**：`content` 欄位為空字串，但 `usage.completion_tokens` 顯示已使用大量 tokens
- **解決方案**：新增 `reasoning_effort: "low"` 參數
  - 降低推理強度
  - 確保有足夠 tokens 用於實際內容生成
- **測試結果**：
  - 修改前：content = ''，reasoning_tokens = 200/200
  - 修改後：content = 有內容，reasoning_tokens = 少量（如 51 tokens）
- **已完成**：
  - 更新 `llm_router.py` 的 `chat()` 和 `tool_call()` 方法
  - 提交到 git（commit fcd9090）
  - 重啟 SHC 服務並驗證成功
- **參考資料**：
  - https://github.com/openai/openai-python/issues/2546
  - https://community.openai.com/t/how-to-get-reasoning-summary-using-gpt-5-mini-in-agent-sdk/1358227


### 進度更新
- [11:07] 修改 SHC llm_router.py 支援 GPT-5 API（max_completion_tokens + reasoning_effort）
- [11:07] Git commit: `b96fad9` + `fcd9090`
- [11:23] 確認 GPT-5 mini 透過 SHC Proxy 正常輸出
- [11:30] 發現 vLLM URL 設定錯誤（指向 localhost 但 vLLM 在 ac-3090）
- [11:38] 查閱知識庫取得正確 vLLM 啟動方式
- [11:40] 在 ac-3090 重啟 vLLM（`--host 127.0.0.1`, `--gpu-memory-utilization 0.75`）
- [11:41] 確認 SSH tunnel systemd service 已運行（`ssh-tunnel-3090-vllm.service`）
- [11:44] SHC VLLM_BASE_URL 改回 localhost:8000
- [11:44] HealthMonitor 確認：`unknown → healthy (vLLM 呼叫成功)`

### 當前架構
```
SHC Proxy (acmacmini2:8081)
├── HIGH tier → GPT-5 mini (OpenAI 外部 API)
├── LOW tier  → Qwen2.5-7B-Instruct (vLLM @ ac-3090)
└── Fallback  → openai → vllm
              └── SSH tunnel: acmacmini2:8000 → ac-3090:127.0.0.1:8000
```

- **狀態**: ✅ 混合模式運作正常

## [11:55] SHC M6 混合模式品質測試規劃

### 現有測試腳本分析

| 腳本 | 模型 | 呼叫方式 | 問題 |
|------|------|---------|------|
| `generate_10_via_shc.py` | SHC 路由 (4.1-nano) | SHC Proxy | 只輸出 7 頁 |
| `compare_models.py` | 多模型 | 直接 OpenAI | 不支援 GPT-5 參數 |
| `generate_with_4o_mini.py` | 4o-mini | 直接 OpenAI | 繞過 SHC |
| `test_gpt5_mini_direct.py` | 5-mini | 直接 OpenAI | 空回應問題 |

### 問題
1. 沒有統一腳本用 SHC Proxy 混合模式測試
2. Token 紀錄不完整（缺 reasoning_tokens）
3. 沒有品質評分機制
4. 現在 SHC 已改為 GPT-5 mini + vLLM 但測試腳本未更新

### 新測試腳本規劃：`test_hybrid_quality.py`

**目標**：透過 SHC Proxy 測試混合模式的完整品質

**測試內容**：
1. 透過 SHC Proxy 呼叫（不繞過），驗證真實混合路由
2. 記錄完整 token：input/output/reasoning/cost
3. 品質評分：頁數、sections 數量、版型多樣性、內容字數
4. 輸出結構化報告：JSON + Markdown

**品質指標**：
- 頁數達標率（目標 25-30 頁）
- 版型多樣性（6 種版型使用比例）
- 內容深度（bullets 平均字數、speaker_notes 字數）
- JSON 解析成功率
- 生成速度與成本

**測試矩陣**：
- 10 個主題 × 3 種風格
- 記錄 SHC 回傳的 model/backend/llm_usage

## [12:20] SHC GPT-5 mini 長 prompt 空內容問題修復

### 問題
測試腳本 `test_hybrid_quality.py` 呼叫 SHC Proxy 持續返回 500 錯誤。

### 根因分析（三輪修復）

**第一輪**：`max_completion_tokens=4096`（SHC 預設值）
- GPT-5 mini reasoning tokens 消耗全部 4096 → content 為空
- 修復：提高到 `max(use_max, 8192)`

**第二輪**：`max_completion_tokens=8192` 仍不夠
- 長 prompt（3645 tokens）時，reasoning 消耗全部 8192 tokens
- 同時 vLLM fallback 失敗：`max_tokens=2048` + prompt 3474 > max-model-len 4096

**第三輪**（最終修復）：
- GPT-5：`max_completion_tokens = max(use_max, 16384)` — 給足 16K 空間
- vLLM：移除固定 `max_tokens`，讓 vLLM 自動用剩餘 context 空間
- 結果：reasoning 只用 128 tokens (1.3%)，content 正常輸出 9685 tokens

### 額外發現
- `OPENAI_TIMEOUT` 預設 60s，.env 已設 180s 但需確保 SHC 重啟載入
- systemd unit 沒有 `EnvironmentFile=`，靠 python-dotenv `load_dotenv()` 載入

### Git commits
- `3f50a32`: GPT-5 max=8192 + vLLM max=2048 限制
- `052864c`: GPT-5 max=16384 + vLLM 移除固定 max_tokens

### 單一主題測試結果
| 指標 | 結果 |
|------|------|
| 品質評分 | 94.6/100 |
| Reasoning tokens | 128 (1.3%) |
| Content tokens | 9,685 |
| PPTX 頁數 | 31 頁 / 6 種版型 |
| 成本 | $0.0206 |
| 耗時 | 111 秒 |

### 完整 10 主題測試結果 ✅

| 指標 | 結果 |
|------|------|
| 成功率 | **10/10 (100%)** |
| 綜合評分 | **94.1/100** |
| 平均 Sections | 30.2 (目標 25) |
| 平均頁數 | 25.1 (目標 30) |
| 版型多樣性 | 100% (6 種版型) |
| Bullet 平均字數 | 36.8 |
| 講稿平均字數 | 121.6 |
| Reasoning tokens | 1,728 (1.8%) |
| 總成本 | $0.2014 (平均 $0.0201/篇) |
| 平均耗時 | 115.8 秒 |
| 模型 | 全部 gpt-5-mini via openai |

### 注意事項
- 有 2 篇 PPTX 顯示 0 頁/0 KB（JSON 解析成功但 PPTX 生成可能失敗）
- 所有請求走 OpenAI（HIGH tier），未觸發 vLLM fallback
- vLLM 作為 fallback 對長 prompt 不適用（max-model-len 4096 不夠）

### 狀態：✅ 完成

### 修改檔案
- `acmacmini2:~/workshop/super-happy-coder/llm_router.py` — GPT-5 支援
- `acmacmini2:~/workshop/super-happy-coder/.env` — OPENAI_TIMEOUT=180
- `acmacmini2:~/hybrid-test/test_hybrid_quality.py` — 測試腳本
- `~/knowledge-base/tech/devops/shc-openai-api-compatibility.md` — 知識庫
- `~/super-happy-tests/test_hybrid_quality.py` — 測試腳本（原始複本）

